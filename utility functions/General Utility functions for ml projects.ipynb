{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27683c9",
   "metadata": {},
   "source": [
    "# null values check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting null values\n",
    "def nullvalues(data):\n",
    "    ls = [] \n",
    "    for i in data.columns:\n",
    "        if data[i].isnull().sum() > 0:\n",
    "            ls.append([i, data[i].isnull().sum()])\n",
    "    if len(ls) != 0:\n",
    "        print('Null values are present in ', ls)\n",
    "        for i in ls:  # getting Percentage Null values and their datatype\n",
    "            nullpercent = i[1]/data.shape[0]\n",
    "            nullpercent = round(nullpercent,6)\n",
    "            dt = type(data[i[0]][1])\n",
    "            print(f'Null percentage in {i[0]} is {nullpercent * 100} % and datatype {dt}')\n",
    "            return ls\n",
    "    else: print('No Null values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b48159",
   "metadata": {},
   "source": [
    "# Box Cox Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "cols = [] # list of numerical columns to be normalized\n",
    "for i in cols:\n",
    "    df_train[i] = df_train[i].apply(lambda x: x + 1) # to remove 0 values from data,  if negative data is present proceed accordingly\n",
    "    df_test[i] = df_test[i].apply(lambda x: x + 1)\n",
    "    df_train[i], fit_lambda = stats.boxcox(df_train[i])\n",
    "    df_test[i] = stats.boxcox(df_test[i], fit_lambda) # transform test data with same lambda that is fitted on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9774e8",
   "metadata": {},
   "source": [
    "# Analysis of regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "def analyse(model, x, y, label):\n",
    "    y_pred = model.predict(x)\n",
    "    residuals =  y_pred - y\n",
    "    \n",
    "    rmse = sqrt(mean_squared_error(y, y_pred))\n",
    "    print('Model RMSE:',round(rmse, 4))\n",
    "\n",
    "    r2=r2_score(y, y_pred)\n",
    "    print('Model r2_score:',round(r2, 4))\n",
    "\n",
    "\n",
    "    fig , ax = plt.subplots(1,3, figsize = (15,5))\n",
    "    plt.suptitle(f'Model analysis for {label} data', fontsize=30)\n",
    "    ax[0].set_title('Error Distribution')\n",
    "    ax[0].set_xlabel('Error Values')\n",
    "    p1 = sns.histplot(residuals, kde = True, bins = 50, ax = ax[0])\n",
    "    \n",
    "    ax[1].set_title('actual vs predictions')\n",
    "    ax[1].set_ylabel('predictons')\n",
    "    ax[1].set_xlabel('actual')\n",
    "    p2 = sns.scatterplot(y, y_pred, ax = ax[1])\n",
    "    ax[2].set_title('Residual plot')\n",
    "    \n",
    "    p3 = sns.scatterplot(y, residuals, ax = ax[2])\n",
    "    ax[2].set_ylabel('residuals')\n",
    "    ax[2].set_xlabel(y.columns[0])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207698f",
   "metadata": {},
   "source": [
    "# Analysis of binary Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebdb402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Function For analysing model\n",
    "def analyse(model, x_data, y_data, data_title):\n",
    "    pred = model.predict(x_data)\n",
    "    cm = confusion_matrix(y_data, pred)\n",
    "    TP = cm[1,1]\n",
    "    TN = cm[0,0]\n",
    "    FP = cm[0,1]\n",
    "    FN = cm[1,0]\n",
    "    print(f'For {data_title} data')\n",
    "    Accuracy  = (accuracy_score(y_data, pred))\n",
    "    Precision  = (TP/(TP+FP))  \n",
    "    Sensitivity =(TP/(TP+FN))\n",
    "    Specificity = (TN/(TN+FP))\n",
    "    F1 = 2*(TP/(TP+FN)*(TP/(TP+FP))/((TP/(TP+FP))+(TP/(TP+FN))))\n",
    "    mat = pd.DataFrame(cm, index=[ 'Actual Negative','Actual Positive'], \n",
    "                                 columns=[ 'Predict Negative','Predict Positive'])\n",
    "    \n",
    "    print('Correct Predictions:' ,TP+TN)\n",
    "    print('False Positives:', FP)\n",
    "    print('False Negatives:', FN)\n",
    "    print('Accuracy:',Accuracy)\n",
    "    print('Precision:',Precision)\n",
    "    print('Sensitivity:',Sensitivity)\n",
    "    print('Specificity:',Specificity)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    \n",
    "    mat = pd.DataFrame(cm, index=[ 'Actual Negative','Actual Positive'], \n",
    "                                 columns=[ 'Predict Negative','Predict Positive'])\n",
    "    y_pred = model.predict_proba(x_data)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_data, y_pred)\n",
    "    print('ROC AUC score' ,roc_auc_score(y_data, y_pred))\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(1,2, figsize = (14,5))\n",
    "    axs[0].set_title('Confusion Matrix')\n",
    "    sns.heatmap(mat, annot = True, fmt = 'd', ax = axs[0])\n",
    "    axs[1].set_title('Roc Auc Curve')\n",
    "    sns.lineplot([0,1], [0,1], ax = axs[1], palette = 'rocket_r')\n",
    "    sns.lineplot(fpr, tpr, ax = axs[1], palette = 'rocket_r' )\n",
    "    axs[1].set_xlabel('False Positive Rate')\n",
    "    axs[1].set_ylabel('True Positive Rate')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7afed",
   "metadata": {},
   "source": [
    "# Feature importance scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ce6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance scoring\n",
    "import xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def modelfit(alg, x, y, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if type(alg) == xgboost.sklearn.XGBClassifier:\n",
    "        if useTrainCV:\n",
    "            xgb_param = alg.get_xgb_params()\n",
    "            xgtrain = xgb.DMatrix(x.values, label=y.values)\n",
    "            cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'],\n",
    "                          nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "            print(cvresult)\n",
    "            alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(x, y)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(x)\n",
    "    dtrain_predprob = alg.predict_proba(x)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print( \"Accuracy : %.4g\" % metrics.accuracy_score(y.values, dtrain_predictions))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, dtrain_predprob))\n",
    "    \n",
    "    \n",
    "    if type(alg) == sklearn.linear_model._logistic.LogisticRegression:\n",
    "        # get importance\n",
    "        importance = alg.coef_[0]\n",
    "        feat_imp=pd.DataFrame({\n",
    "        'columns':x.columns,\n",
    "        'importance':importance})\n",
    "        feat_imp.sort_values(by='importance', ascending=False)\n",
    "        # plot feature importance\n",
    "        plt.figure(figsize = (20,10))\n",
    "        sns.barplot(x = 'columns', y = 'importance', data = feat_imp)\n",
    "        plt.xticks(rotation=90)\n",
    "    \n",
    "    if type(alg) == sklearn.linear_model._base.LinearRegression:\n",
    "        # get importance\n",
    "        importance = alg.coef_\n",
    "        feat_imp=pd.DataFrame({\n",
    "        'columns':x.columns,\n",
    "        'importance':importance})\n",
    "        feat_imp.sort_values(by='importance', ascending=False)\n",
    "        # plot feature importance\n",
    "        plt.figure(figsize = (20,10))\n",
    "        sns.barplot(x = 'columns', y = 'importance', data = feat_imp)\n",
    "        plt.xticks(rotation=90)\n",
    "        \n",
    "    if type(alg) in [sklearn.tree._classes.DecisionTreeClassifier, xgboost.sklearn.XGBClassifier,\n",
    "                     sklearn.ensemble._forest.RandomForestClassifier]:\n",
    "        importance = alg.feature_importances_.argsort()\n",
    "        importance = importance[::-1]\n",
    "        feat_imp=pd.DataFrame({\n",
    "        'cols':x.columns[importance],\n",
    "        'imps':alg.feature_importances_[importance]})\n",
    "        # plot feature importance\n",
    "        plt.figure(figsize = (50,20))\n",
    "        sns.barplot(x = 'cols', y = 'imps', data = feat_imp)\n",
    "        plt.xticks(rotation=90)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e4684",
   "metadata": {},
   "source": [
    "# hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b4ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function For hyper parameter tuning of the model\n",
    "from sklearn.model_selection import GridSearchCV   #Performing grid search\n",
    "def bestvalues(model, x, y, parameters ,cvrounds = 5, scoring = 'roc_auc'):\n",
    "    grid_search = GridSearchCV(estimator = model,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = scoring,\n",
    "                           cv = cvrounds,\n",
    "                           verbose=0)\n",
    "    \n",
    "    grid_search.fit(x,y)\n",
    "    # best score achieved during the GridSearchCV\n",
    "    print('GridSearch CV best score : {:.4f}\\n\\n'.format(grid_search.best_score_))\n",
    "\n",
    "# print parameters that give the best results\n",
    "    print('Parameters that give the best results :','\\n\\n', (grid_search.best_params_))\n",
    "\n",
    "# print estimator that was chosen by the GridSearch\n",
    "    print('\\n\\nEstimator that was chosen by the search :','\\n\\n', (grid_search.best_estimator_))\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57416d9a",
   "metadata": {},
   "source": [
    "# VIF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF analysis\n",
    "from statsmodels.stats.outliers_influence  import  variance_inflation_factor \n",
    "def get_VIF(X_train):\n",
    " # A dataframe that will contain the names of all the feature variables and their respective VIFs \n",
    "    vif = pd.DataFrame() \n",
    "    vif['Features'] = X_train.columns \n",
    "    vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])] \n",
    "    #vif['VIF'] = (vif['VIF'], 2) \n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False) \n",
    "    return (vif)\n",
    "\n",
    "vif_ = pd.DataFrame(get_VIF( )) # put training data here\n",
    "vif_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616adbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb87b1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
